data_xviii <- scrape_legislature_data(url_xviii)
print(data_xviii)
url <- "https://it.wikipedia.org/wiki/XIX_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
library(polite)
library(rvest)
library(dplyr)
library(openxlsx)
# Function to scrape data from the specific table on a Wikipedia page
scrape_legislature_data <- function(url) {
# Create a polite session
session <- bow(url)
# Scrape the page
page <- scrape(session)
# Find the table with the class "infobox sinottico"
table_node <- page %>%
html_node(xpath = '//table[contains(@class, "infobox sinottico")]')
if (is.null(table_node)) {
stop("Table with class 'infobox sinottico' not found.")
}
# Extract the section titles and their corresponding party rows
headings <- table_node %>%
html_nodes("tr th") %>%
html_text(trim = TRUE)
# Find the indices of section headings
camera_index <- which(grepl("Camera dei deputati", headings, ignore.case = TRUE))
senato_index <- which(grepl("Senato della repubblica", headings, ignore.case = TRUE))
# Function to extract party data from a row
extract_party_info <- function(row, section_title) {
# Extract the <a> tag if it exists
a_tag <- row %>% html_node("th a")
if (is.null(a_tag)) {
return(NULL)  # Skip rows without party information
}
# Extract party name
party_name <- a_tag %>%
html_text(trim = TRUE)
# Extract hyperlink
link <- a_tag %>%
html_attr("href")
link <- ifelse(is.na(link), NA, paste0("https://it.wikipedia.org", link))
# Extract title attribute
title <- a_tag %>%
html_attr("title")
# Extract number of seats
seats_div <- row %>%
html_node("td div span.nowrap")
seats <- if (!is.null(seats_div)) {
seats_div %>% html_text(trim = TRUE)
} else {
NA
}
# Extract color from nested div structure
color_div <- row %>%
html_node(xpath = ".//td/div/div/div[contains(@style, 'background-color')]")
color_style <- if (!is.null(color_div)) {
color_div %>% html_attr("style")
} else {
NA
}
color <- if (!is.na(color_style)) {
sub(".*background-color:([^;]+);.*", "\\1", color_style)
} else {
NA
}
# Check if all fields are missing
if (is.na(party_name) & is.na(link) & is.na(title) & is.na(seats) & is.na(color)) {
return(NULL)  # Skip rows with all fields missing
}
# Return a row of the dataframe with the section title
data.frame(
party = party_name,
link = link,
title = title,
seats = seats,
background_color = color,
Camera = if (section_title == "Camera dei deputati") section_title else NA,
Senato = if (section_title == "Senato della repubblica") section_title else NA,
stringsAsFactors = FALSE
)
}
# Extract and combine all party data
table_data <- lapply(seq_along(rows), function(i) {
row <- rows[[i]]
section_title <- NA
# Check which section the row belongs to
for (index in c(camera_index, senato_index)) {
if (i > index) {
section_title <- headings[index]
}
}
extract_party_info(row, section_title)
})
final_table <- do.call(rbind, table_data)
# Remove rows with all NA fields in Camera and Senato
final_table <- final_table %>%
filter(!is.na(Camera) | !is.na(Senato))
return(final_table)
}
# Example usage
url <- "https://it.wikipedia.org/wiki/XIX_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
session <- bow(url)
View(session)
page <- scrape(session)
View(page)
table_node <- page %>%
html_node(xpath = '//table[contains(@class, "infobox sinottico")]')
View(table_node)
headings <- table_node %>%
html_nodes("tr th") %>%
html_text(trim = TRUE)
library(polite)
library(rvest)
library(dplyr)
library(openxlsx)
# Function to scrape data from the specific table on a Wikipedia page
scrape_legislature_data <- function(url) {
# Create a polite session
session <- bow(url)
# Scrape the page
page <- scrape(session)
# Find the table with the class "infobox sinottico"
table_node <- page %>%
html_node(xpath = '//table[contains(@class, "infobox sinottico")]')
if (is.null(table_node)) {
stop("Table with class 'infobox sinottico' not found.")
}
# Extract the section titles and their corresponding party rows
headings <- table_node %>%
html_nodes("tr th") %>%
html_text(trim = TRUE)
# Find the indices of section headings
camera_index <- which(grepl("Camera dei deputati", headings, ignore.case = TRUE))
senato_index <- which(grepl("Senato della repubblica", headings, ignore.case = TRUE))
# Function to extract party data from a row
extract_party_info <- function(row, section_title) {
# Extract the <a> tag if it exists
a_tag <- row %>% html_node("th a")
if (is.null(a_tag)) {
return(NULL)  # Skip rows without party information
}
# Extract party name
party_name <- a_tag %>%
html_text(trim = TRUE)
# Extract hyperlink
link <- a_tag %>%
html_attr("href")
link <- ifelse(is.na(link), NA, paste0("https://it.wikipedia.org", link))
# Extract title attribute
title <- a_tag %>%
html_attr("title")
# Extract number of seats
seats_div <- row %>%
html_node("td div span.nowrap")
seats <- if (!is.null(seats_div)) {
seats_div %>% html_text(trim = TRUE)
} else {
NA
}
# Extract color from nested div structure
color_div <- row %>%
html_node(xpath = ".//td/div/div/div[contains(@style, 'background-color')]")
color_style <- if (!is.null(color_div)) {
color_div %>% html_attr("style")
} else {
NA
}
color <- if (!is.na(color_style)) {
sub(".*background-color:([^;]+);.*", "\\1", color_style)
} else {
NA
}
# Check if all fields are missing
if (is.na(party_name) & is.na(link) & is.na(title) & is.na(seats) & is.na(color)) {
return(NULL)  # Skip rows with all fields missing
}
# Return a row of the dataframe with the section title
data.frame(
party = party_name,
link = link,
title = title,
seats = seats,
background_color = color,
Camera = if (section_title == "Camera dei deputati") section_title else NA,
Senato = if (section_title == "Senato della repubblica") section_title else NA,
stringsAsFactors = FALSE
)
}
# Extract and combine all party data
table_data <- lapply(seq_along(rows), function(i) {
row <- rows[[i]]
section_title <- NA
# Check which section the row belongs to
for (index in c(camera_index, senato_index)) {
if (i > index) {
section_title <- headings[index]
}
}
extract_party_info(row, section_title)
})
final_table <- do.call(rbind, table_data)
# Remove rows with all NA fields in Camera and Senato
final_table <- final_table %>%
filter(!is.na(Camera) | !is.na(Senato))
return(final_table)
}
# Example usage
url <- "https://it.wikipedia.org/wiki/XIX_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
# Load required packages
library(polite)
library(rvest)
library(dplyr)
# Function to scrape data from the specific table on a Wikipedia page
scrape_legislature_data <- function(url) {
# Create a polite session
session <- bow(url)
# Scrape the page
page <- scrape(session)
# Find the table with the class "infobox sinottico"
table_node <- page %>%
html_node(xpath = '//table[contains(@class, "infobox sinottico")]')
if (is.null(table_node)) {
stop("Table with class 'infobox sinottico' not found.")
}
# Extract the party information from the table
rows <- table_node %>%
html_nodes("tr")
# Function to extract party data from a row
extract_party_info <- function(row) {
# Extract the <a> tag if it exists
a_tag <- row %>% html_node("th a")
if (is.null(a_tag)) {
return(NULL)  # Skip rows without party information
}
# Extract party name
party_name <- a_tag %>%
html_text(trim = TRUE)
# Extract hyperlink
link <- a_tag %>%
html_attr("href")
link <- ifelse(is.na(link), NA, paste0("https://it.wikipedia.org", link))
# Extract title attribute
title <- a_tag %>%
html_attr("title")
# Extract number of seats
seats_div <- row %>%
html_node("td div span.nowrap")
seats <- if (!is.null(seats_div)) {
seats_div %>% html_text(trim = TRUE)
} else {
NA
}
# Extract color from nested div structure
color_div <- row %>%
html_node(xpath = ".//td/div/div/div[contains(@style, 'background-color')]")
color_style <- if (!is.null(color_div)) {
color_div %>% html_attr("style")
} else {
NA
}
color <- if (!is.na(color_style)) {
sub(".*background-color:([^;]+);.*", "\\1", color_style)
} else {
NA
}
# Check if all fields are missing
if (is.na(party_name) & is.na(link) & is.na(title) & is.na(seats) & is.na(color)) {
return(NULL)  # Skip rows with all fields missing
}
# Return a row of the dataframe
data.frame(
party = party_name,
link = link,
title = title,
seats = seats,
background_color = color,
stringsAsFactors = FALSE
)
}
# Extract and combine all party data
table_data <- lapply(rows, extract_party_info)
final_table <- do.call(rbind, table_data)
return(final_table)
}
# Example usage
url_xix <- "https://it.wikipedia.org/wiki/XIX_legislatura_della_Repubblica_Italiana"
data_xix <- scrape_legislature_data(url_xix)
print(data_xix)
url_xviii <- "https://it.wikipedia.org/wiki/XVIII_legislatura_della_Repubblica_Italiana"
data_xviii <- scrape_legislature_data(url_xviii)
print(data_xviii)
# Example usage
url <- "https://it.wikipedia.org/wiki/XIX_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
shiny::runApp()
runApp()
install.packages("tidyverse")
shiny::runApp()
runApp()
url <- "https://it.wikipedia.org/wiki/XVIII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XVIII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
# Load required packages
library(polite)
library(rvest)
library(dplyr)
# Function to scrape data from the specific table on a Wikipedia page
scrape_legislature_data <- function(url) {
# Create a polite session
session <- bow(url)
# Scrape the page
page <- scrape(session)
# Find the table with the class "infobox sinottico"
table_node <- page %>%
html_node(xpath = '//table[contains(@class, "infobox sinottico")]')
if (is.null(table_node)) {
stop("Table with class 'infobox sinottico' not found.")
}
# Extract the party information from the table
rows <- table_node %>%
html_nodes("tr")
# Function to extract party data from a row
extract_party_info <- function(row) {
# Extract the <a> tag if it exists
a_tag <- row %>% html_node("th a")
if (is.null(a_tag)) {
return(NULL)  # Skip rows without party information
}
# Extract party name
party_name <- a_tag %>%
html_text(trim = TRUE)
# Extract hyperlink
link <- a_tag %>%
html_attr("href")
link <- ifelse(is.na(link), NA, paste0("https://it.wikipedia.org", link))
# Extract title attribute
title <- a_tag %>%
html_attr("title")
# Extract number of seats
seats_div <- row %>%
html_node("td div span.nowrap")
seats <- if (!is.null(seats_div)) {
seats_div %>% html_text(trim = TRUE)
} else {
NA
}
# Extract color from nested div structure
color_div <- row %>%
html_node(xpath = ".//td/div/div/div[contains(@style, 'background-color')]")
color_style <- if (!is.null(color_div)) {
color_div %>% html_attr("style")
} else {
NA
}
color <- if (!is.na(color_style)) {
sub(".*background-color:([^;]+);.*", "\\1", color_style)
} else {
NA
}
# Check if all fields are missing
if (is.na(party_name) & is.na(link) & is.na(title) & is.na(seats) & is.na(color)) {
return(NULL)  # Skip rows with all fields missing
}
# Return a row of the dataframe
data.frame(
party = party_name,
link = link,
title = title,
seats = seats,
background_color = color,
stringsAsFactors = FALSE
)
}
# Extract and combine all party data
table_data <- lapply(rows, extract_party_info)
final_table <- do.call(rbind, table_data)
return(final_table)
}
url <- "https://it.wikipedia.org/wiki/XVIII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
library(readxl)
write.xlsx(data_paties, 'data_paties.xlsx')
library(openxlsx)
url <- "https://it.wikipedia.org/wiki/XVIII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XVII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XVI_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XV_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XVI_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XV_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XIV_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XIII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/XI_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/X_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/X_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/IX_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/VIII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/VII_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/VI_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/V_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/IV_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/III_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/II_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
url <- "https://it.wikipedia.org/wiki/I_legislatura_della_Repubblica_Italiana"
data_paties <- scrape_legislature_data(url)
print(data_paties)
write.xlsx(data_paties, 'data_paties.xlsx')
parties <- read_xlsx(path = "data.xlsx", sheet = "PARTITI")
View(parties)
parties[rep(row.names(parties), df$Seggi), ]
parties[rep(df$Seggi), ]
df %>%
map_df(., rep, .$Seggi)
parties[rep(row.names(parties), parties$Seggi), ]
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
traceback()
runApp()
